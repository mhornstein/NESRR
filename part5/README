The regressor.py Script
=======================
The regressor.py script is designed to train a deep network architecture for the purpose of performing regression.
Its goal is to predict the mutual information (MI) between two masked entities in a sentence.
This prediction relies on two factors:
1 - The sentence embedding.
2 - The assumption made by the networks regarding the labels assigned to the two provided masked entities according to the embedings.
The network runs a classification task for it. The labels can be: PERSON, FAC, ORG, GPE, LOC, PRODUCT, EVENT, WORK_OF_ART, or LAW.
To complete this task, the script requires two inputs:
1 - The dataset created in part 1.
2 - The pre-prepared embeddings of the sentences created in part 4.

How to run the script?
======================
In the command prompt, type: python regressor.py <path to dataset> <path to embeddings file>
for example: python regressor.py C:\Users\User\Documents\data.csv C:\Users\User\Documents\embeddings.out

Script output
=============
* The progress will be logged to the console.
* The "result" directory will be generated to store the results, which includes the following files:
- avg_train_loss.jpg: a plot illustrating the loss of the training set over the epochs.
- avg_val_loss.jpg: a plot illustrating the loss of the validation set over the epochs.
- epoch_time.jpg: a plot illustrating the duration of each epoch.
- results.csv: a CSV file containing the aforementioned information in a textual format.

More configurations
===================
* Network configuration: The network configuration can be customized by modifying the following constants:
1 - CLASSIFICATION_NETWORK_HIDDEN_LAYERS_CONFIG - for the part of the network in charge of labels classification.
2 - REGRESSION_NETWORK_HIDDEN_LAYERS_CONFIG - for the part of the network in charge of the regression task.
Both of these constants are lists that specifies the configuration of the hidden layers in the network.
The format of the list is as follows: [hidden_dim_1, dropout_rate_1, hidden_dim_2, dropout_rate_2, ...].
To indicate the absence of a dropout layer, you can use the value None.
For example, [512, 0.1, 128, None] represents a network with a hidden dense layer of size 512,
followed by a dropout layer with a rate of 0.1, and then another dense layer of size 128 without a dropout layer afterwards.
Note: The networks input and output shapes are predefined by the task and therefore cannot be modified or adjusted.
* Batch size, learning rate, and number of epochs can be adjusted by updating the corresponding constants: BATCH_SIZE, LEARNING_RATE, and NUM_EPOCHS, respectively.
* You can modify the MI_TRANSFORMATION constant to apply a transformation to the MI score before training the network.
This transformation can be one of the following options: 'sqrt', 'ln', 'log10', or None (for no transformation).

--------------------------------------------------------------------------------------------------------

Environment
===========
The scripts were tested in the following config:
OS: Windows
Python version: 3.10.8
Transformers version: 4.24.0
Torch version: 1.13.0+cpu
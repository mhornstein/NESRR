The data_preprocessor.py Script
===============================
The data_preprocessor.py script loads the WikiText-103 dataset files, extract its lines (each line represents a paragraph)
and revert any preprocessing done to the original text (such as tokenization or escaping using the @ symbol).

How to run the script?
======================
1. Download the WikiText-103 dataset and unzip it. The unzipped directory will contain 3 files: wiki.train.raw, wiki.valid.raw and wiki.test.raw.
2. In the cmd, write: python data_preprocessor.py <path to the unzipped directory>.
for example: python data_preprocessor.py C:\Users\User\Documents\wikitext-103-raw

Script output
=============
* The progress will be logged to the console.
* The preprocessed data will be written to the text file: processed_data.txt

Link to WikiText-103 dataset
============================
Dataset description link: https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/
Direct link to download the dataset: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip?ref=blog.salesforceairesearch.com

--------------------------------------------------------------------------------------------------------

The data_extractor.py script
============================
The data_extractor.py script loads the preprocessed WikiText-103 data, extracts its named entities and generates the required
dataset.

How to run the script?
======================
In the cmd, write: python data_extractor.py <path to the preprocessed data file>.
for example: python data_extractor.py C:\Users\User\Documents\processed_data.txt

In general, the processed_data.txt is expected to be a text file where each line consists of a paragraph representing one or more sentences.

Script output
=============
* The progress and data measurements will be logged to the console.
* original_data_stats directory will be created with csv files and plots of stats of the complete data.
* sampled_data_stats directory will be created with csv files and plots of stats of the sampled data (as sampled for the final dataset).
* The sampled dataset will be created under the file 'data.csv'

More configurations
===================
One can further adjust other script parameters by updating the K, N_PROCESS, TEXT_BATCH_SIZE, N and DATASET_FILE constants.
Their roles are documented in the script beside them.

--------------------------------------------------------------------------------------------------------

Environment
===========
The scripts were tested in the following config:
OS: Windows
Python version: 3.10.8
Spacy version: 3.5.3 with en_core_web_lg model installed

--------------------------------------------------------------------------------------------------------

Note: The "data_processor_output" and "data_extractor_output" directories
=========================================================================
This directories contain the output of running the above scripts.
Note that as the files were big, they were committed using Git Large File Storage (Git LFS).
To clone the repository with this files, you need to have Git LFS installed.